# -*- coding: utf-8 -*-
"""dataprep_featureengineering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IRSuoKEs2jTLVGeWWzEjBadZpWENGj8s
"""

import pandas as pd
import numpy as np
import ipaddress
import re
import seaborn as sn
from sklearn.preprocessing import LabelEncoder


import matplotlib.pyplot as plt

from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
from sklearn.ensemble import IsolationForest
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.decomposition import PCA

from google.colab import data_table
data_table.enable_dataframe_formatter()

from google.colab import auth
auth.authenticate_user()
print('Authenticated')

"""# Connecting to Bigquery"""

#@title Ambil Data Cloud/BigQuery Project ID
project_id = 'stately-node-363801' #@param{type:"string"}

# Package used for interfacing w/ BigQuery from Python
from google.cloud import bigquery

# Create BigQuery client
bq_client = bigquery.Client(project = project_id)

# Commented out IPython magic to ensure Python compatibility.
# %%bigquery network_data --project {project_id}
# 
# SELECT
# *
# FROM `stately-node-363801.network.cleaned_data`

network_data

print(network_data.isnull().sum())

percent_missing =network_data.isnull().sum() / len(network_data) *100
rounded_percent = round(network_data.isnull().sum() / len(network_data) *100)
print(percent_missing)

print(rounded_percent)

print(network_data.isnull().sum())

"""# Feature Engineering

**categorical column =  ['StartTime', 'Proto', 'SrcAddr', 'Sport', 'Dir', 'DstAddr', 'Dport', 'State', 'Label']**
- menggunakan label encoder -> proto, dir,  state, label
- menggunakan konsep convert IP address menjadi integer -> SrcAddr, DstAddr
- conversi menjadi tipe data integer -> Sport, Dport

starttime

## convert to integer

terdapat beberapa port yang tidak valid, maka selain konversi ke dalam bentuk integer juga melakukan penggantian value terhadap data invalid
- yang bisa dikonversi: hexadecimal string, "0x...." https://www.quora.com/Is-it-possible-to-have-a-memory-address-starting-with-0x0x  
- https://www.geeksforgeeks.org/convert-ip-address-to-integer-and-vice-versa/
"""

df1 = network_data.copy()
df1.head()

#memastikan tidak ada data kosong
print(df1.isnull().sum())

# Function to convert port to integer, handling hexadecimal and other non-numeric values
def convert_port(port):
    try:
        # convert langsung ke integer
        return int(port)
    except ValueError:
        try:
            # kalau konversi gagal, cek apakah hexadecimal string
            if isinstance(port, str) and port.startswith("0x"):
                return int(port, 16)
        except ValueError:
            pass
        # Return None or a default value for non-numeric values
        return None

# menerapkan hasilnya ke dataframe
df1['destination_port'] = df1['destination_port'].apply(convert_port)
df1['source_port'] = df1['source_port'].apply(convert_port)
# df["Dport"] = pd.to_numeric(df["Dport"])


# Alternatif, kalau masih ada yang kosong diisi 0
df1['destination_port'] = df1['destination_port'].fillna(0)
df1['source_port'] = df1['source_port'].fillna(0)

df1.info()

df1.head()

"""## label encoding

akan menerapkan label encoder pada -> proto, dir,  state, label
"""

columns = ['protocol', 'direction', 'state', 'label']
df_label=df1.copy()
df_label[columns].describe()

print(df_label.isnull().sum())

"""### Proto"""

label_encoder = LabelEncoder()

# Encode PROTO

df_label['protocol'] = label_encoder.fit_transform(df_label['protocol'])

# Create separate DataFrames for mappings
df_proto_mapping = pd.DataFrame({
    'protocol (Categorical Value)': label_encoder.classes_,
    'protocol (Numeric Value)': label_encoder.transform(label_encoder.classes_)
})

# Display the mappings DataFrames
print("Mapping for column 'protocol':")
print(df_proto_mapping)

"""### Dir"""

# Encode Dir

df_label['direction'] = label_encoder.fit_transform(df_label['direction'])

# Create separate DataFrames for mappings
df_dir_mapping = pd.DataFrame({
    'direction (Categorical Value)': label_encoder.classes_,
    'direction (Numeric Value)': label_encoder.transform(label_encoder.classes_)
})

# Display the mappings DataFrames
print("\nMapping for column 'direction':")
print(df_dir_mapping)

"""### State"""

# Encode State

df_label['state'] = label_encoder.fit_transform(df_label['state'])

df_state_mapping = pd.DataFrame({
    'state (Categorical Value)': label_encoder.classes_,
    'state (Numeric Value)': label_encoder.transform(label_encoder.classes_)
})
print("\nMapping for column 'state':")
print(df_state_mapping)

"""### Label"""

network_data['label'].unique()

df_label['label'] = label_encoder.fit_transform(df_label['label'])

df_label_mapping = pd.DataFrame({
    'label (Categorical Value)': label_encoder.classes_,
    'label (Numeric Value)': label_encoder.transform(label_encoder.classes_)
})
print("\nMapping for column 'label':")
print(df_label_mapping)

df_label.head()

df_label.info()

"""### Convert StartTime"""

df_time = df_label.copy()
df_time.head()

print(df_time.isnull().sum())

# Convert 'StartTime' to datetime format
df_time['start_time'] = pd.to_datetime(df_time['start_time'], format='%M:%S.%f')

# Extract hour, minute, and second as separate features
df_time['Minute'] = df_time['start_time'].dt.minute
df_time['Second'] = df_time['start_time'].dt.second
df_time['Microsecond'] = df_time['start_time'].dt.microsecond
df_time = df_time.drop('start_time', axis=1)

df_time.head()

"""## convert ip address"""

df_ip = df_time.copy()
df_ip.head()

def ip_to_int(ip):
    try:
        # Convert IP to integer safely
        return int(ipaddress.ip_address(ip))
    except ValueError:
        # Handle invalid IP addresses
        return None
    except OverflowError:
        # Handle overflow errors
        return None

# Apply the function to the DataFrame
df_ip['source_address'] = df_ip['source_address'].apply(ip_to_int)
df_ip['destination_address'] = df_ip['destination_address'].apply(ip_to_int)

df_ip.head()

df_ip.info()

print(df_ip.isnull().sum())

df1['source_address'] = df1['destination_port'].fillna(0)

df_ip['source_address'] = pd.to_numeric(df_ip['source_address'], errors='coerce')
df_ip['destination_address'] = pd.to_numeric(df_ip['destination_address'], errors='coerce')

df_ip.info()

"""## Save Clean Data"""

df_ip.head()

#@title Write All Data to BigQuery Table
output_dataset_id = 'network' #@param{type:'string'}

output_table_id = 'clean_data' #@param{type:'string'}

replace_or_append_output = 'replace' #@param{type:'string'} ['replace', 'append']

# Combine project and dataset
project_dataset = f"{bq_client.project}.{output_dataset_id}"

# Combine project, dataset, and table
project_dataset_table = f"{project_dataset}.{output_table_id}"

# Check to make sure output dataset exists, create it if not
try:
  bq_client.get_dataset(output_dataset_id)
  print(f"BigQuery dataset {project_dataset} exists\n")

except:
  print(f"BigQuery dataset {project_dataset} doesn't exist, so creating it\n")
  dataset = bq_client.create_dataset(bigquery.Dataset(project_dataset))

job_config = bigquery.LoadJobConfig()

# Modify job config depending on if we want to replace or append to table
if(replace_or_append_output == 'replace'):
  job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE
else:
  job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND

dataset_ref = bq_client.dataset(output_dataset_id)
table_ref = dataset_ref.table(output_table_id)

# Use client functionality to load BigQuery table from Pandas data frame
bq_client.load_table_from_dataframe(
  dataframe = df_ip,
  destination = table_ref,
  job_config = job_config
  ).result()

print((f"Sample Station Fit Info output ({replace_or_append_output}) to "
  f"BigQuery table {project_dataset_table}\n"))

"""# Feature Selection, with Heatmap Correlation

## Without Splitting Data
"""

df_fin = df_ip.copy()
df_fin.head()

df_fin.rename(columns={
    'Dur': 'duration',
    'Proto': 'protocol',
    'SrcAddr': 'source_address',
    'Sport': 'source_port',
    'Dir': 'direction',
    'DstAddr': 'destination_address',
    'Dport': 'destination_port',
    'State': 'state',
    'sTos': 'source_tos',
    'dTos': 'destination_tos',
    'TotPkts': 'total_packets',
    'TotBytes': 'total_bytes',
    'SrcBytes': 'source_bytes',
    'Label': 'label'
}, inplace=True)

# Calculate the correlation matrix
correlation_matrix = df_fin.corr()

# Create the heatmap
plt.figure(figsize = (10,8))
sn.heatmap(correlation_matrix, cmap = 'coolwarm')
plt.show()

#use for anomaly detection is = label, duration, protocol, source_port, direction, destination_port, state